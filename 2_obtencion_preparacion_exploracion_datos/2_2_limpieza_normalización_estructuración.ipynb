{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUNAPILLCO/neural_profit/blob/main/2_obtencion_preparacion_exploracion_datos/2_2_limpieza_normalizaci%C3%B3n_estructuraci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa4bfae",
      "metadata": {
        "id": "8fa4bfae"
      },
      "source": [
        "# 2_1_Limpieza, Normalizaci√≥n y estructuraci√≥n de series temporales\n",
        "# Preprocesamiento de Datos del √çndice E-mini Nasdaq 100 (MNQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Clonamos el repositorio"
      ],
      "metadata": {
        "id": "VGThNZbqZtjD"
      },
      "id": "VGThNZbqZtjD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINK DE REPOSITORIO: https://github.com/GUNAPILLCO/neural_profit"
      ],
      "metadata": {
        "id": "BFIvYsI1acw9"
      },
      "id": "BFIvYsI1acw9"
    },
    {
      "cell_type": "code",
      "source": [
        "#Clonamos el repo\n",
        "!git clone https://github.com/GUNAPILLCO/neural_profit.git"
      ],
      "metadata": {
        "id": "yJHa7R8gZsx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01802499-3497-4e56-c0d9-71dc75b44b86"
      },
      "id": "yJHa7R8gZsx7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neural_profit'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 37 (delta 0), reused 34 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37/37), 40.88 MiB | 21.17 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5888009",
      "metadata": {
        "id": "e5888009"
      },
      "source": [
        "## 1. Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -q pandas_market_calendars\n",
        "print(\"‚úÖ Librer√≠a instalada: pandas_market_calendars\")"
      ],
      "metadata": {
        "id": "RuOOQhe-ntbE",
        "outputId": "426109e0-2e35-48af-e1ec-963a3eac988a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RuOOQhe-ntbE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/123.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/200.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Librer√≠a instalada: pandas_market_calendars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "614e74ca",
      "metadata": {
        "id": "614e74ca"
      },
      "outputs": [],
      "source": [
        "# Utilidades generales\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Manejo y procesamiento de datos\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Calendario de mercados\n",
        "import pandas_market_calendars as mcal\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265ae912",
      "metadata": {
        "id": "265ae912"
      },
      "source": [
        "## 2. Contexto y fuente de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf4589d",
      "metadata": {
        "id": "abf4589d"
      },
      "source": [
        "Los datos corresponden al contrato MNQ (Micro E-mini Nasdaq 100) descargados desde NinjaTrader con frecuencia de un minuto (formato OHLCV).\n",
        "\n",
        "- Open: precio de apertura\n",
        "- High: precio m√°ximo\n",
        "- Low: precio m√≠nimo\n",
        "- Close: precio de cierre\n",
        "- Volume: volumen negociado\n",
        "\n",
        "Los datos est√°n en la zona horaria UTC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ab5ec9",
      "metadata": {
        "id": "95ab5ec9"
      },
      "source": [
        "## 3. Generaci√≥n de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8122f5c",
      "metadata": {
        "id": "c8122f5c"
      },
      "source": [
        "Dado que los contratos se encuentran almacenados en archivos .txt dentro de la carpeta historicos_mnq, es necesario unificarlos en un √∫nico dataset consolidado.\n",
        "\n",
        "La siguiente funci√≥n se encarga de leer los archivos .txt, asignar nombres a las columnas correspondientes y establecer la columna datetime como √≠ndice temporal del dataframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_df ():\n",
        "\n",
        "    # Ruta a los archivos .txt\n",
        "    ruta_historicos = '/content/neural_profit/1_historicos_mnq/*.txt'  # Reemplace con su ruta local\n",
        "\n",
        "    # Lista para almacenar DataFrames individuales\n",
        "    df_mnq = []\n",
        "\n",
        "    # Leer todos los archivos .txt\n",
        "    for archivo in glob.glob(ruta_historicos):\n",
        "        df = pd.read_csv(\n",
        "            archivo,\n",
        "            sep=';',\n",
        "            header=None,\n",
        "            names=['datetime', 'open', 'high', 'low', 'close', 'volume'],\n",
        "            dtype={'open': float, 'high': float, 'low': float, 'close': float, 'volume': int}\n",
        "        )\n",
        "\n",
        "        # Convertir columna 'datetime' al formato datetime real\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], format='%Y%m%d %H%M%S')\n",
        "\n",
        "        # Establecer como √≠ndice\n",
        "        df.set_index('datetime', inplace=True)\n",
        "\n",
        "        df_mnq.append(df)\n",
        "\n",
        "    # Unir todos los DataFrames\n",
        "    df_mnq_raw = pd.concat(df_mnq)\n",
        "    # Ordenar por fecha si es necesario\n",
        "    df_mnq_raw.sort_index(inplace=True)\n",
        "\n",
        "    return df_mnq_raw"
      ],
      "metadata": {
        "id": "8_VNw1LwjYJB"
      },
      "id": "8_VNw1LwjYJB",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e82ebcdd",
      "metadata": {
        "id": "e82ebcdd"
      },
      "source": [
        "El siguiente bloque de c√≥digo verifica si el dataset consolidado ya ha sido generado previamente.\n",
        "\n",
        "En particular, comprueba la existencia del archivo mnq_raw_data.parquet.\n",
        "\n",
        "- Si el archivo est√° presente, se carga directamente en la variable df_mnq.\n",
        "\n",
        "- En caso contrario, se invoca la funci√≥n generate_dataset() para generar el dataset a partir de los archivos originales."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_o_generar_df():\n",
        "\n",
        "    archivo = '/content/neural_profit/2_obtencion_preparacion_exploracion_datos/mnq_raw_data.parquet'\n",
        "\n",
        "    if os.path.exists(archivo):\n",
        "        print(\"üìÇ Archivo encontrado en disco. Cargando dataset local...\")\n",
        "        df_mnq_raw = pd.read_parquet(archivo)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Archivo no encontrado en GitHub. Generando dataset desde archivos hist√≥ricos...\")\n",
        "        df_mnq_raw = generar_df()\n",
        "        df_mnq_raw.to_parquet(archivo, index=True)\n",
        "        print(\"‚úÖ Dataset generado y guardado localmente.\")\n",
        "\n",
        "    return df_mnq_raw"
      ],
      "metadata": {
        "id": "P1jEK088jkZJ"
      },
      "id": "P1jEK088jkZJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mnq_raw = cargar_o_generar_df()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2ax_TwukPNH",
        "outputId": "5ad05efb-fbc0-4a79-d8c2-bb353e4a3785"
      },
      "id": "L2ax_TwukPNH",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Archivo encontrado en disco. Cargando dataset local...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504382cf",
      "metadata": {
        "id": "504382cf"
      },
      "source": [
        "## 4. Filtrado de d√≠as no h√°biles y horario burs√°til"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23cdad4",
      "metadata": {
        "id": "f23cdad4"
      },
      "source": [
        "### 4.1. Filtrado de fines de semana y feriados burs√°tiles estadounidenses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d44470",
      "metadata": {
        "id": "43d44470"
      },
      "source": [
        "Es necesario filtrar del conjunto de datos aquellas filas correspondientes a s√°bados, domingos y feriados burs√°tiles. Para ello, se utilizar√° la librer√≠a pandas_market_calendars, que permite identificar los d√≠as h√°biles de operaci√≥n seg√∫n el calendario oficial del NASDAQ.\n",
        "\n",
        "La funci√≥n implementada filtra un DataFrame con √≠ndice de tipo DatetimeIndex, conservando √∫nicamente aquellas filas cuya fecha coincida con un d√≠a h√°bil del mercado. La marca temporal completa (fecha y hora) se mantiene sin modificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7aa8f82",
      "metadata": {
        "id": "a7aa8f82"
      },
      "outputs": [],
      "source": [
        "def filtrar_dias_habiles_nasdaq(df):\n",
        "    # Crear el calendario del mercado NASDAQ\n",
        "    nasdaq = mcal.get_calendar('NASDAQ')\n",
        "\n",
        "    # Obtener el rango de fechas del √≠ndice del DataFrame\n",
        "    start_date = df.index.min().date()\n",
        "    end_date = df.index.max().date()\n",
        "\n",
        "    # Obtener el cronograma de d√≠as h√°biles del mercado\n",
        "    valid_dates = nasdaq.schedule(start_date=start_date, end_date=end_date).index.date\n",
        "\n",
        "    # Filtrar el DataFrame verificando si la fecha de cada marca temporal est√° en los d√≠as v√°lidos\n",
        "    df_filtrado = df[df.index.normalize().isin(valid_dates)]\n",
        "\n",
        "    return df_filtrado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b91f804",
      "metadata": {
        "id": "9b91f804"
      },
      "outputs": [],
      "source": [
        "df_mnq = filtrar_dias_habiles_nasdaq (df_mnq_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3030c8ac",
      "metadata": {
        "id": "3030c8ac"
      },
      "source": [
        "### 4.2. Filtrado de horario de operaci√≥n de mercado de New York (09:30 a 16:00) con pre mercado, desde las 07:30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503a687c",
      "metadata": {
        "id": "503a687c"
      },
      "source": [
        "Dado que los timestamps del √≠ndice (DatetimeIndex) provienen de archivos .txt sin informaci√≥n de zona horaria, es necesario indicar expl√≠citamente a pandas que dichos valores est√°n en formato UTC.\n",
        "\n",
        "Una vez establecido el timezone, se procede a convertir los timestamps desde UTC a la hora local del mercado estadounidense (zona US/Eastern), correspondiente a los horarios de operaci√≥n del NASDAQ/NYSE. Esta conversi√≥n se realiza teniendo en cuenta autom√°ticamente los ajustes por horario de verano o invierno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73820485",
      "metadata": {
        "id": "73820485"
      },
      "outputs": [],
      "source": [
        "def configurar_zona_horaria(df, from_tz='UTC', to_tz='America/New_York'):\n",
        "    \"\"\"\n",
        "    Asegura que el √≠ndice del DataFrame tenga zona horaria 'from_tz'\n",
        "    y lo convierte a la zona horaria 'to_tz'.\n",
        "    \"\"\"\n",
        "    if df.index.tz is None:\n",
        "        # Localiza en from_tz si no tiene zona horaria\n",
        "        df.index = df.index.tz_localize(from_tz)\n",
        "    # Convierte a la zona horaria deseada\n",
        "    df.index = df.index.tz_convert(to_tz)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3598a884",
      "metadata": {
        "id": "3598a884"
      },
      "outputs": [],
      "source": [
        "df_mnq = configurar_zona_horaria(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb43243",
      "metadata": {
        "id": "6fb43243"
      },
      "source": [
        "La siguiente funci√≥n selecciona √∫nicamente las muestras que se encuentran dentro del horario regular de operaci√≥n burs√°til del NASDAQ.\n",
        "\n",
        "Filtra un DataFrame cuyo √≠ndice es de tipo DatetimeIndex, conservando solo aquellas filas cuya marca temporal se encuentre entre las 09:30 y 16:00 horas (US/Eastern), correspondientes al horario de negociaci√≥n est√°ndar en d√≠as h√°biles de mercado.\n",
        "\n",
        "Particularmente, decido agregar una hora de pre mercado, desde las 07:30AM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a07ebb0e",
      "metadata": {
        "id": "a07ebb0e"
      },
      "outputs": [],
      "source": [
        "def filtrar_horas_habiles_nasdaq(df):\n",
        "\n",
        "    # Filtrar solo las filas dentro de las horas de mercado (de 7:30 AM a 4:00 PM)\n",
        "    df_filtered = df.between_time('07:30:00', '16:00:00')\n",
        "\n",
        "    # Retornar el DataFrame filtrado\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1514117c",
      "metadata": {
        "id": "1514117c"
      },
      "outputs": [],
      "source": [
        "df_mnq = filtrar_horas_habiles_nasdaq(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb71ac27",
      "metadata": {
        "id": "bb71ac27"
      },
      "source": [
        "## 5. An√°lisis de registros diarios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eed73d4",
      "metadata": {
        "id": "4eed73d4"
      },
      "source": [
        "Es necesario verificar que todos los d√≠as del conjunto de datos contengan la misma cantidad de registros y que estos sean consecutivos, es decir, que no falte ning√∫n minuto dentro de cada jornada.\n",
        "\n",
        "La funci√≥n analizar_registros_por_dia permite realizar este control sobre un DataFrame con √≠ndice de tipo datetime. La funci√≥n contabiliza la cantidad de registros por d√≠a e imprime una tabla resumen que indica cu√°ntos d√≠as presentan una determinada cantidad de registros. Esto resulta √∫til para identificar inconsistencias, como d√≠as incompletos o interrupciones en la frecuencia temporal esperada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6b03a076",
      "metadata": {
        "id": "6b03a076"
      },
      "outputs": [],
      "source": [
        "def analizar_registros_por_dia(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Analiza la cantidad de registros por d√≠a en un DataFrame con √≠ndice datetime.\n",
        "\n",
        "    Imprime:\n",
        "    - Distribuci√≥n de la cantidad de registros por d√≠a.\n",
        "    - Porcentaje de d√≠as con menos registros que el valor m√°s frecuente.\n",
        "\n",
        "    Retorna:\n",
        "    - Serie con el conteo de registros por cada d√≠a.\n",
        "    \"\"\"\n",
        "    # Contar la cantidad de registros por d√≠a\n",
        "    conteo_diario = df.groupby(df.index.date).size()\n",
        "\n",
        "    # Calcular la distribuci√≥n de registros por d√≠a\n",
        "    distribucion = conteo_diario.value_counts().sort_index(ascending=False)\n",
        "    tabla = [[registros, cantidad_dias] for registros, cantidad_dias in distribucion.items()]\n",
        "\n",
        "    print(\"Distribuci√≥n de cantidad de registros por d√≠a:\\n\")\n",
        "    print(tabulate(tabla, headers=[\"Registros por d√≠a\", \"Cantidad de d√≠as\"], tablefmt=\"grid\"))\n",
        "\n",
        "    # Determinar el valor m√°s frecuente de registros por d√≠a\n",
        "    registros_dia_completo = conteo_diario.mode().iloc[0]\n",
        "    print(f\"\\nCantidad de registros en un d√≠a completo: {registros_dia_completo}\")\n",
        "\n",
        "    # Calcular el porcentaje de d√≠as incompletos\n",
        "    total_dias = len(conteo_diario)\n",
        "    dias_con_menos = (conteo_diario < registros_dia_completo).sum()\n",
        "    porcentaje = (dias_con_menos / total_dias) * 100\n",
        "\n",
        "    print(f\"D√≠as con menos de {registros_dia_completo} registros: {dias_con_menos} de {total_dias} ({porcentaje:.2f}%)\")\n",
        "\n",
        "    return conteo_diario, registros_dia_completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "15b8e1dd",
      "metadata": {
        "id": "15b8e1dd",
        "outputId": "086beb93-a6e5-4772-ee51-3d2bb2470592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de cantidad de registros por d√≠a:\n",
            "\n",
            "+---------------------+--------------------+\n",
            "|   Registros por d√≠a |   Cantidad de d√≠as |\n",
            "+=====================+====================+\n",
            "|                 511 |               1198 |\n",
            "+---------------------+--------------------+\n",
            "|                 510 |                  8 |\n",
            "+---------------------+--------------------+\n",
            "|                 509 |                  4 |\n",
            "+---------------------+--------------------+\n",
            "|                 508 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 504 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 503 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 502 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 500 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 499 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 458 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 432 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 404 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 394 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 383 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 346 |                  9 |\n",
            "+---------------------+--------------------+\n",
            "|                 344 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 122 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 121 |                  9 |\n",
            "+---------------------+--------------------+\n",
            "|                 120 |                  3 |\n",
            "+---------------------+--------------------+\n",
            "|                 119 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 118 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 117 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 116 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 109 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 108 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "\n",
            "Cantidad de registros en un d√≠a completo: 511\n",
            "D√≠as con menos de 511 registros: 56 de 1254 (4.47%)\n"
          ]
        }
      ],
      "source": [
        "resumen, registros_dia_completo = analizar_registros_por_dia(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cef1314",
      "metadata": {
        "id": "5cef1314"
      },
      "source": [
        "Como podemos observar en la tabla, la gran mayor√≠a de d√≠as tienen `511` muestras. Y representan m√°s del 95% del total de los datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aee7dba",
      "metadata": {
        "id": "3aee7dba"
      },
      "source": [
        "### 5.1. Filtrado de d√≠as incompletos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc60a994",
      "metadata": {
        "id": "fc60a994"
      },
      "source": [
        "La siguiente funci√≥n encuentra los indices de las fechas con registros incompletos (menor a 511):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "58057ccf",
      "metadata": {
        "id": "58057ccf"
      },
      "outputs": [],
      "source": [
        "def busqueda_fechas_incompletas(df: pd.DataFrame, gap_minutes: int = 1, registros_esperados: int = registros_dia_completo) -> list:\n",
        "    \"\"\"\n",
        "    Muestra una tabla con los d√≠as que tienen menos de los registros esperados o presentan irregularidades temporales.\n",
        "    Retorna una lista con esas fechas.\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame con √≠ndice datetime.\n",
        "    - gap_minutes: intervalo esperado entre registros consecutivos (en minutos).\n",
        "    - registros_esperados: cantidad esperada de registros por d√≠a.\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de fechas (datetime.date) con menos registros de los esperados o problemas temporales.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_diff'] = df.index.to_series().diff()\n",
        "    base_time_diff = pd.Timedelta(minutes=gap_minutes)\n",
        "\n",
        "    conteos = df.groupby(df.index.date).size()\n",
        "    fechas_problema = []\n",
        "\n",
        "    for date, group in df.groupby(df.index.date):\n",
        "        time_diff = group['time_diff'].iloc[1:]\n",
        "        tiene_irregularidades = (time_diff != base_time_diff).any()\n",
        "        cantidad = conteos[date]\n",
        "\n",
        "        if cantidad < registros_esperados or tiene_irregularidades:\n",
        "            fechas_problema.append((date, cantidad))\n",
        "\n",
        "    '''\n",
        "    if fechas_problema:\n",
        "        print(f\"\\nD√≠a con menos de {registros_esperados} registros o con problemas temporales:\\n\")\n",
        "        print(f\"{'+' + '-'*21 + '+' + '-'*20 + '+'}\")\n",
        "        print(f\"| {'Fecha'.ljust(19)} | {'Registros'.rjust(18)} |\")\n",
        "        print(f\"{'+' + '='*21 + '+' + '='*20 + '+'}\")\n",
        "        for fecha, registros in fechas_problema:\n",
        "            print(f\"| {str(fecha).ljust(19)} | {str(registros).rjust(18)} |\")\n",
        "            print(f\"{'+' + '-'*21 + '+' + '-'*20 + '+'}\")\n",
        "    else:\n",
        "        print(\"No se encontraron d√≠as con irregularidades ni registros incompletos.\")\n",
        "    '''\n",
        "    # Solo devolver las fechas\n",
        "    return [fecha for fecha, _ in fechas_problema]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f354689",
      "metadata": {
        "id": "3f354689"
      },
      "source": [
        "Elimino las fechas con registros incompletos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "53934801",
      "metadata": {
        "id": "53934801",
        "outputId": "0bf6f2bd-fc94-4e5f-9680-4d5dae918131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de cantidad de registros por d√≠a:\n",
            "\n",
            "+---------------------+--------------------+\n",
            "|   Registros por d√≠a |   Cantidad de d√≠as |\n",
            "+=====================+====================+\n",
            "|                 511 |               1198 |\n",
            "+---------------------+--------------------+\n",
            "\n",
            "Cantidad de registros en un d√≠a completo: 511\n",
            "D√≠as con menos de 511 registros: 0 de 1198 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Filtrar eliminando las fechas con problemas\n",
        "df_mnq = df_mnq[~df_mnq.index.to_series().dt.date.isin(busqueda_fechas_incompletas(df_mnq))]\n",
        "\n",
        "#Y verifico con:\n",
        "resumen=analizar_registros_por_dia(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5be8e7",
      "metadata": {
        "id": "2e5be8e7"
      },
      "source": [
        "## 6. Verificaci√≥n de continuidad temporal minuto a minuto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e80f60",
      "metadata": {
        "id": "23e80f60"
      },
      "source": [
        "Es necesario verificar que los 451 registros correspondientes a un mismo d√≠a est√©n dispuestos de forma consecutiva, con una separaci√≥n exacta de un minuto entre cada muestra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2c5a40c1",
      "metadata": {
        "id": "2c5a40c1"
      },
      "outputs": [],
      "source": [
        "def detectar_gaps(df: pd.DataFrame, gap_minutes: int = 1):\n",
        "    \"\"\"\n",
        "    Verifica si existen saltos mayores al intervalo esperado (por defecto 1 minuto)\n",
        "    entre registros consecutivos dentro de cada d√≠a, en un DataFrame con √≠ndice tipo DatetimeIndex.\n",
        "\n",
        "    Omite el primer registro de cada d√≠a.\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame con √≠ndice datetime.\n",
        "    - gap_minutes: tama√±o esperado del intervalo en minutos (por defecto 1).\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de √≠ndices donde se detectaron diferencias mayores al intervalo esperado.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_diff'] = df.index.to_series().diff()\n",
        "\n",
        "    base_time_diff = pd.Timedelta(minutes=gap_minutes)\n",
        "    problem_indices = []\n",
        "\n",
        "    for date, group in df.groupby(df.index.date):\n",
        "        time_diff = group['time_diff'].iloc[1:]\n",
        "        incorrect_indices = time_diff[time_diff != base_time_diff].index\n",
        "        if len(incorrect_indices) > 0:\n",
        "            problem_indices.append(incorrect_indices)\n",
        "\n",
        "    if problem_indices:\n",
        "        print(f\"Se encontraron problemas en {len(problem_indices)} registros con diferencias irregulares.\\n\")\n",
        "\n",
        "        # Conteo por fecha\n",
        "        conteos = df.groupby(df.index.date).size()\n",
        "\n",
        "        for i in range(len(problem_indices)):\n",
        "            idx = problem_indices[i][0]\n",
        "            diff = df.loc[idx, 'time_diff']\n",
        "            date = idx.date()\n",
        "            count = conteos[date]\n",
        "            print(f'\\t{idx} -> Diferencia: {diff} | # Registros: {count}')\n",
        "    else:\n",
        "        print(\"No se encontraron problemas, todas las muestras son consecutivas minuto a minuto.\")\n",
        "\n",
        "    return problem_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9136b7ba",
      "metadata": {
        "id": "9136b7ba",
        "outputId": "74f21389-4b47-4fac-a952-1d39008654b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se encontraron problemas, todas las muestras son consecutivas minuto a minuto.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "detectar_gaps(df_mnq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "95bf2523",
      "metadata": {
        "id": "95bf2523",
        "outputId": "27abfc58-9189-4693-ac01-77fb7854057e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 612178 entries, 2019-12-23 07:30:00-05:00 to 2024-12-27 16:00:00-05:00\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   open    612178 non-null  float64\n",
            " 1   high    612178 non-null  float64\n",
            " 2   low     612178 non-null  float64\n",
            " 3   close   612178 non-null  float64\n",
            " 4   volume  612178 non-null  int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 28.0 MB\n"
          ]
        }
      ],
      "source": [
        "df_mnq.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df_mnq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nOZ80nM5Rc0N",
        "outputId": "0d6f9af5-3d07-465c-f4f6-f078e0c26ce5"
      },
      "id": "nOZ80nM5Rc0N",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame</b><br/>def __init__(data=None, index: Axes | None=None, columns: Axes | None=None, dtype: Dtype | None=None, copy: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Two-dimensional, size-mutable, potentially heterogeneous tabular data.\n",
              "\n",
              "Data structure also contains labeled axes (rows and columns).\n",
              "Arithmetic operations align on both row and column labels. Can be\n",
              "thought of as a dict-like container for Series objects. The primary\n",
              "pandas data structure.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame\n",
              "    Dict can contain Series, arrays, constants, dataclass or list-like objects. If\n",
              "    data is a dict, column order follows insertion-order. If a dict contains Series\n",
              "    which have an index defined, it is aligned by its index. This alignment also\n",
              "    occurs if data is a Series or a DataFrame itself. Alignment is done on\n",
              "    Series/DataFrame inputs.\n",
              "\n",
              "    If data is a list of dicts, column order follows insertion-order.\n",
              "\n",
              "index : Index or array-like\n",
              "    Index to use for resulting frame. Will default to RangeIndex if\n",
              "    no indexing information part of input data and no index provided.\n",
              "columns : Index or array-like\n",
              "    Column labels to use for resulting frame when data does not have them,\n",
              "    defaulting to RangeIndex(0, 1, 2, ..., n). If data contains column labels,\n",
              "    will perform column selection instead.\n",
              "dtype : dtype, default None\n",
              "    Data type to force. Only a single dtype is allowed. If None, infer.\n",
              "copy : bool or None, default None\n",
              "    Copy data from inputs.\n",
              "    For dict data, the default of None behaves like ``copy=True``.  For DataFrame\n",
              "    or 2d ndarray input, the default of None behaves like ``copy=False``.\n",
              "    If data is a dict containing one or more Series (possibly of different dtypes),\n",
              "    ``copy=False`` will ensure that these inputs are not copied.\n",
              "\n",
              "    .. versionchanged:: 1.3.0\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.from_records : Constructor from tuples, also record arrays.\n",
              "DataFrame.from_dict : From dicts of Series, arrays, or dicts.\n",
              "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
              "read_table : Read general delimited file into DataFrame.\n",
              "read_clipboard : Read text from clipboard into DataFrame.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "Please reference the :ref:`User Guide &lt;basics.dataframe&gt;` for more information.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "Constructing DataFrame from a dictionary.\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [1, 2], &#x27;col2&#x27;: [3, 4]}\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d)\n",
              "&gt;&gt;&gt; df\n",
              "   col1  col2\n",
              "0     1     3\n",
              "1     2     4\n",
              "\n",
              "Notice that the inferred dtype is int64.\n",
              "\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int64\n",
              "col2    int64\n",
              "dtype: object\n",
              "\n",
              "To enforce a single dtype:\n",
              "\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=d, dtype=np.int8)\n",
              "&gt;&gt;&gt; df.dtypes\n",
              "col1    int8\n",
              "col2    int8\n",
              "dtype: object\n",
              "\n",
              "Constructing DataFrame from a dictionary including Series:\n",
              "\n",
              "&gt;&gt;&gt; d = {&#x27;col1&#x27;: [0, 1, 2, 3], &#x27;col2&#x27;: pd.Series([2, 3], index=[2, 3])}\n",
              "&gt;&gt;&gt; pd.DataFrame(data=d, index=[0, 1, 2, 3])\n",
              "   col1  col2\n",
              "0     0   NaN\n",
              "1     1   NaN\n",
              "2     2   2.0\n",
              "3     3   3.0\n",
              "\n",
              "Constructing DataFrame from numpy ndarray:\n",
              "\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n",
              "...                    columns=[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;])\n",
              "&gt;&gt;&gt; df2\n",
              "   a  b  c\n",
              "0  1  2  3\n",
              "1  4  5  6\n",
              "2  7  8  9\n",
              "\n",
              "Constructing DataFrame from a numpy ndarray that has labeled columns:\n",
              "\n",
              "&gt;&gt;&gt; data = np.array([(1, 2, 3), (4, 5, 6), (7, 8, 9)],\n",
              "...                 dtype=[(&quot;a&quot;, &quot;i4&quot;), (&quot;b&quot;, &quot;i4&quot;), (&quot;c&quot;, &quot;i4&quot;)])\n",
              "&gt;&gt;&gt; df3 = pd.DataFrame(data, columns=[&#x27;c&#x27;, &#x27;a&#x27;])\n",
              "...\n",
              "&gt;&gt;&gt; df3\n",
              "   c  a\n",
              "0  3  1\n",
              "1  6  4\n",
              "2  9  7\n",
              "\n",
              "Constructing DataFrame from dataclass:\n",
              "\n",
              "&gt;&gt;&gt; from dataclasses import make_dataclass\n",
              "&gt;&gt;&gt; Point = make_dataclass(&quot;Point&quot;, [(&quot;x&quot;, int), (&quot;y&quot;, int)])\n",
              "&gt;&gt;&gt; pd.DataFrame([Point(0, 0), Point(0, 3), Point(2, 3)])\n",
              "   x  y\n",
              "0  0  0\n",
              "1  0  3\n",
              "2  2  3\n",
              "\n",
              "Constructing DataFrame from Series/DataFrame:\n",
              "\n",
              "&gt;&gt;&gt; ser = pd.Series([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df = pd.DataFrame(data=ser, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df\n",
              "   0\n",
              "a  1\n",
              "c  3\n",
              "\n",
              "&gt;&gt;&gt; df1 = pd.DataFrame([1, 2, 3], index=[&quot;a&quot;, &quot;b&quot;, &quot;c&quot;], columns=[&quot;x&quot;])\n",
              "&gt;&gt;&gt; df2 = pd.DataFrame(data=df1, index=[&quot;a&quot;, &quot;c&quot;])\n",
              "&gt;&gt;&gt; df2\n",
              "   x\n",
              "a  1\n",
              "c  3</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 509);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be37b70",
      "metadata": {
        "id": "9be37b70"
      },
      "source": [
        "## 7. Guardado de dataset final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6901eb",
      "metadata": {
        "id": "0b6901eb"
      },
      "source": [
        "Se guarda un dataset compuesto por 1198 d√≠as, cada uno con 511 registros correspondientes a minutos consecutivos.\n",
        "\n",
        "El conjunto de datos incluye √∫nicamente d√≠as h√°biles de operaci√≥n burs√°til, y abarca el intervalo horario comprendido entre las 07:30 y las 16:00 horas (US/Eastern)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardamos el dataset\n",
        "ruta_mnq_intraday = '/content/neural_profit/2_obtencion_preparacion_exploracion_datos/mnq_intraday_data.parquet'\n",
        "df_mnq.to_parquet(ruta_mnq_intraday, index=True)"
      ],
      "metadata": {
        "id": "ks7D-6wsT839"
      },
      "id": "ks7D-6wsT839",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos de directorio\n",
        "%cd neural_profit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqs_Th8Mmtev",
        "outputId": "35dae696-9dca-4607-cb75-98b4fbb72ade"
      },
      "id": "nqs_Th8Mmtev",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neural_profit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = \" \" #busca el archivo token en D:\\Escritorio\\UBA-CEIA\\token_neural_profit\n",
        "!git config --global user.email \"gusunapillco@gmail.com\"\n",
        "!git config --global user.name \"GUNAPILLCO\"\n",
        "!git add .\n",
        "!git commit -m \"Actualizaci√≥n de datasets desde colab\"\n",
        "!git push https://GUNAPILLCO:{token}@github.com/GUNAPILLCO/neural_profit.git\n",
        "\n",
        "print(\"Dataset guardados correctamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnoC50l5malj",
        "outputId": "c85f60be-7166-4d49-ee39-1d284915888e"
      },
      "id": "nnoC50l5malj",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 6f1cbab] Actualizaci√≥n de notebooks y datasets desde colab\n",
            " 2 files changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 2_obtencion_preparacion_exploracion_datos/mnq_intraday_data.parquet\n",
            " create mode 100644 2_obtencion_preparacion_exploracion_datos/mnq_raw_data.parquet\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 29.94 MiB | 4.55 MiB/s, done.\n",
            "Total 5 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/GUNAPILLCO/neural_profit.git\n",
            "   aab5afd..6f1cbab  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "trader_IA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}