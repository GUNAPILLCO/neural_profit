{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GUNAPILLCO/neural_profit/blob/main/2_obtencion_preparacion_exploracion_datos/2_2_limpieza_normalizaci%C3%B3n_estructuraci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fa4bfae",
      "metadata": {
        "id": "8fa4bfae"
      },
      "source": [
        "# 2_1_Limpieza, Normalizaci√≥n y estructuraci√≥n de series temporales\n",
        "# Preprocesamiento de Datos del √çndice E-mini Nasdaq 100 (MNQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Clonamos el repositorio"
      ],
      "metadata": {
        "id": "VGThNZbqZtjD"
      },
      "id": "VGThNZbqZtjD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINK DE REPOSITORIO: https://github.com/GUNAPILLCO/neural_profit"
      ],
      "metadata": {
        "id": "BFIvYsI1acw9"
      },
      "id": "BFIvYsI1acw9"
    },
    {
      "cell_type": "code",
      "source": [
        "#Clonamos el repo\n",
        "!git clone https://github.com/GUNAPILLCO/neural_profit.git"
      ],
      "metadata": {
        "id": "yJHa7R8gZsx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01802499-3497-4e56-c0d9-71dc75b44b86"
      },
      "id": "yJHa7R8gZsx7",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neural_profit'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 37 (delta 0), reused 34 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (37/37), 40.88 MiB | 21.17 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5888009",
      "metadata": {
        "id": "e5888009"
      },
      "source": [
        "## 1. Importaci√≥n de Librer√≠as"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -q pandas_market_calendars\n",
        "print(\"‚úÖ Librer√≠a instalada: pandas_market_calendars\")"
      ],
      "metadata": {
        "id": "RuOOQhe-ntbE",
        "outputId": "426109e0-2e35-48af-e1ec-963a3eac988a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RuOOQhe-ntbE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/123.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/200.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Librer√≠a instalada: pandas_market_calendars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "614e74ca",
      "metadata": {
        "id": "614e74ca"
      },
      "outputs": [],
      "source": [
        "# Utilidades generales\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Manejo y procesamiento de datos\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Calendario de mercados\n",
        "import pandas_market_calendars as mcal\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "265ae912",
      "metadata": {
        "id": "265ae912"
      },
      "source": [
        "## 2. Contexto y fuente de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf4589d",
      "metadata": {
        "id": "abf4589d"
      },
      "source": [
        "Los datos corresponden al contrato MNQ (Micro E-mini Nasdaq 100) descargados desde NinjaTrader con frecuencia de un minuto (formato OHLCV).\n",
        "\n",
        "- Open: precio de apertura\n",
        "- High: precio m√°ximo\n",
        "- Low: precio m√≠nimo\n",
        "- Close: precio de cierre\n",
        "- Volume: volumen negociado\n",
        "\n",
        "Los datos est√°n en la zona horaria UTC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ab5ec9",
      "metadata": {
        "id": "95ab5ec9"
      },
      "source": [
        "## 3. Generaci√≥n de dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8122f5c",
      "metadata": {
        "id": "c8122f5c"
      },
      "source": [
        "Dado que los contratos se encuentran almacenados en archivos .txt dentro de la carpeta historicos_mnq, es necesario unificarlos en un √∫nico dataset consolidado.\n",
        "\n",
        "La siguiente funci√≥n se encarga de leer los archivos .txt, asignar nombres a las columnas correspondientes y establecer la columna datetime como √≠ndice temporal del dataframe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_df ():\n",
        "\n",
        "    # Ruta a los archivos .txt\n",
        "    ruta_historicos = '/content/neural_profit/1_historicos_mnq/*.txt'  # Reemplace con su ruta local\n",
        "\n",
        "    # Lista para almacenar DataFrames individuales\n",
        "    df_mnq = []\n",
        "\n",
        "    # Leer todos los archivos .txt\n",
        "    for archivo in glob.glob(ruta_historicos):\n",
        "        df = pd.read_csv(\n",
        "            archivo,\n",
        "            sep=';',\n",
        "            header=None,\n",
        "            names=['datetime', 'open', 'high', 'low', 'close', 'volume'],\n",
        "            dtype={'open': float, 'high': float, 'low': float, 'close': float, 'volume': int}\n",
        "        )\n",
        "\n",
        "        # Convertir columna 'datetime' al formato datetime real\n",
        "        df['datetime'] = pd.to_datetime(df['datetime'], format='%Y%m%d %H%M%S')\n",
        "\n",
        "        # Establecer como √≠ndice\n",
        "        df.set_index('datetime', inplace=True)\n",
        "\n",
        "        df_mnq.append(df)\n",
        "\n",
        "    # Unir todos los DataFrames\n",
        "    df_mnq_raw = pd.concat(df_mnq)\n",
        "    # Ordenar por fecha si es necesario\n",
        "    df_mnq_raw.sort_index(inplace=True)\n",
        "\n",
        "    return df_mnq_raw"
      ],
      "metadata": {
        "id": "8_VNw1LwjYJB"
      },
      "id": "8_VNw1LwjYJB",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e82ebcdd",
      "metadata": {
        "id": "e82ebcdd"
      },
      "source": [
        "El siguiente bloque de c√≥digo verifica si el dataset consolidado ya ha sido generado previamente.\n",
        "\n",
        "En particular, comprueba la existencia del archivo mnq_raw_data.parquet.\n",
        "\n",
        "- Si el archivo est√° presente, se carga directamente en la variable df_mnq.\n",
        "\n",
        "- En caso contrario, se invoca la funci√≥n generate_dataset() para generar el dataset a partir de los archivos originales."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cargar_o_generar_df():\n",
        "\n",
        "    archivo = '/content/neural_profit/2_obtencion_preparacion_exploracion_datos/mnq_raw_data.parquet'\n",
        "\n",
        "    if os.path.exists(archivo):\n",
        "        print(\"üìÇ Archivo encontrado en disco. Cargando dataset local...\")\n",
        "        df_mnq_raw = pd.read_parquet(archivo)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Archivo no encontrado en GitHub. Generando dataset desde archivos hist√≥ricos...\")\n",
        "        df_mnq_raw = generar_df()\n",
        "        df_mnq_raw.to_parquet(archivo, index=True)\n",
        "        print(\"‚úÖ Dataset generado y guardado localmente.\")\n",
        "\n",
        "    return df_mnq_raw"
      ],
      "metadata": {
        "id": "P1jEK088jkZJ"
      },
      "id": "P1jEK088jkZJ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mnq_raw = cargar_o_generar_df()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2ax_TwukPNH",
        "outputId": "5ad05efb-fbc0-4a79-d8c2-bb353e4a3785"
      },
      "id": "L2ax_TwukPNH",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Archivo encontrado en disco. Cargando dataset local...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504382cf",
      "metadata": {
        "id": "504382cf"
      },
      "source": [
        "## 4. Filtrado de d√≠as no h√°biles y horario burs√°til"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f23cdad4",
      "metadata": {
        "id": "f23cdad4"
      },
      "source": [
        "### 4.1. Filtrado de fines de semana y feriados burs√°tiles estadounidenses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d44470",
      "metadata": {
        "id": "43d44470"
      },
      "source": [
        "Es necesario filtrar del conjunto de datos aquellas filas correspondientes a s√°bados, domingos y feriados burs√°tiles. Para ello, se utilizar√° la librer√≠a pandas_market_calendars, que permite identificar los d√≠as h√°biles de operaci√≥n seg√∫n el calendario oficial del NASDAQ.\n",
        "\n",
        "La funci√≥n implementada filtra un DataFrame con √≠ndice de tipo DatetimeIndex, conservando √∫nicamente aquellas filas cuya fecha coincida con un d√≠a h√°bil del mercado. La marca temporal completa (fecha y hora) se mantiene sin modificaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a7aa8f82",
      "metadata": {
        "id": "a7aa8f82"
      },
      "outputs": [],
      "source": [
        "def filtrar_dias_habiles_nasdaq(df):\n",
        "    # Crear el calendario del mercado NASDAQ\n",
        "    nasdaq = mcal.get_calendar('NASDAQ')\n",
        "\n",
        "    # Obtener el rango de fechas del √≠ndice del DataFrame\n",
        "    start_date = df.index.min().date()\n",
        "    end_date = df.index.max().date()\n",
        "\n",
        "    # Obtener el cronograma de d√≠as h√°biles del mercado\n",
        "    valid_dates = nasdaq.schedule(start_date=start_date, end_date=end_date).index.date\n",
        "\n",
        "    # Filtrar el DataFrame verificando si la fecha de cada marca temporal est√° en los d√≠as v√°lidos\n",
        "    df_filtrado = df[df.index.normalize().isin(valid_dates)]\n",
        "\n",
        "    return df_filtrado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b91f804",
      "metadata": {
        "id": "9b91f804"
      },
      "outputs": [],
      "source": [
        "df_mnq = filtrar_dias_habiles_nasdaq (df_mnq_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3030c8ac",
      "metadata": {
        "id": "3030c8ac"
      },
      "source": [
        "### 4.2. Filtrado de horario de operaci√≥n de mercado de New York (09:30 a 16:00) con pre mercado, desde las 07:30"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503a687c",
      "metadata": {
        "id": "503a687c"
      },
      "source": [
        "Dado que los timestamps del √≠ndice (DatetimeIndex) provienen de archivos .txt sin informaci√≥n de zona horaria, es necesario indicar expl√≠citamente a pandas que dichos valores est√°n en formato UTC.\n",
        "\n",
        "Una vez establecido el timezone, se procede a convertir los timestamps desde UTC a la hora local del mercado estadounidense (zona US/Eastern), correspondiente a los horarios de operaci√≥n del NASDAQ/NYSE. Esta conversi√≥n se realiza teniendo en cuenta autom√°ticamente los ajustes por horario de verano o invierno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "73820485",
      "metadata": {
        "id": "73820485"
      },
      "outputs": [],
      "source": [
        "def configurar_zona_horaria(df, from_tz='UTC', to_tz='America/New_York'):\n",
        "    \"\"\"\n",
        "    Asegura que el √≠ndice del DataFrame tenga zona horaria 'from_tz'\n",
        "    y lo convierte a la zona horaria 'to_tz'.\n",
        "    \"\"\"\n",
        "    if df.index.tz is None:\n",
        "        # Localiza en from_tz si no tiene zona horaria\n",
        "        df.index = df.index.tz_localize(from_tz)\n",
        "    # Convierte a la zona horaria deseada\n",
        "    df.index = df.index.tz_convert(to_tz)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3598a884",
      "metadata": {
        "id": "3598a884"
      },
      "outputs": [],
      "source": [
        "df_mnq = configurar_zona_horaria(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb43243",
      "metadata": {
        "id": "6fb43243"
      },
      "source": [
        "La siguiente funci√≥n selecciona √∫nicamente las muestras que se encuentran dentro del horario regular de operaci√≥n burs√°til del NASDAQ.\n",
        "\n",
        "Filtra un DataFrame cuyo √≠ndice es de tipo DatetimeIndex, conservando solo aquellas filas cuya marca temporal se encuentre entre las 09:30 y 16:00 horas (US/Eastern), correspondientes al horario de negociaci√≥n est√°ndar en d√≠as h√°biles de mercado.\n",
        "\n",
        "Particularmente, decido agregar una hora de pre mercado, desde las 07:30AM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a07ebb0e",
      "metadata": {
        "id": "a07ebb0e"
      },
      "outputs": [],
      "source": [
        "def filtrar_horas_habiles_nasdaq(df):\n",
        "\n",
        "    # Filtrar solo las filas dentro de las horas de mercado (de 7:30 AM a 4:00 PM)\n",
        "    df_filtered = df.between_time('07:30:00', '16:00:00')\n",
        "\n",
        "    # Retornar el DataFrame filtrado\n",
        "    return df_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1514117c",
      "metadata": {
        "id": "1514117c"
      },
      "outputs": [],
      "source": [
        "df_mnq = filtrar_horas_habiles_nasdaq(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb71ac27",
      "metadata": {
        "id": "bb71ac27"
      },
      "source": [
        "## 5. An√°lisis de registros diarios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eed73d4",
      "metadata": {
        "id": "4eed73d4"
      },
      "source": [
        "Es necesario verificar que todos los d√≠as del conjunto de datos contengan la misma cantidad de registros y que estos sean consecutivos, es decir, que no falte ning√∫n minuto dentro de cada jornada.\n",
        "\n",
        "La funci√≥n analizar_registros_por_dia permite realizar este control sobre un DataFrame con √≠ndice de tipo datetime. La funci√≥n contabiliza la cantidad de registros por d√≠a e imprime una tabla resumen que indica cu√°ntos d√≠as presentan una determinada cantidad de registros. Esto resulta √∫til para identificar inconsistencias, como d√≠as incompletos o interrupciones en la frecuencia temporal esperada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "6b03a076",
      "metadata": {
        "id": "6b03a076"
      },
      "outputs": [],
      "source": [
        "def analizar_registros_por_dia(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Analiza la cantidad de registros por d√≠a en un DataFrame con √≠ndice datetime.\n",
        "\n",
        "    Imprime:\n",
        "    - Distribuci√≥n de la cantidad de registros por d√≠a.\n",
        "    - Porcentaje de d√≠as con menos registros que el valor m√°s frecuente.\n",
        "\n",
        "    Retorna:\n",
        "    - Serie con el conteo de registros por cada d√≠a.\n",
        "    \"\"\"\n",
        "    # Contar la cantidad de registros por d√≠a\n",
        "    conteo_diario = df.groupby(df.index.date).size()\n",
        "\n",
        "    # Calcular la distribuci√≥n de registros por d√≠a\n",
        "    distribucion = conteo_diario.value_counts().sort_index(ascending=False)\n",
        "    tabla = [[registros, cantidad_dias] for registros, cantidad_dias in distribucion.items()]\n",
        "\n",
        "    print(\"Distribuci√≥n de cantidad de registros por d√≠a:\\n\")\n",
        "    print(tabulate(tabla, headers=[\"Registros por d√≠a\", \"Cantidad de d√≠as\"], tablefmt=\"grid\"))\n",
        "\n",
        "    # Determinar el valor m√°s frecuente de registros por d√≠a\n",
        "    registros_dia_completo = conteo_diario.mode().iloc[0]\n",
        "    print(f\"\\nCantidad de registros en un d√≠a completo: {registros_dia_completo}\")\n",
        "\n",
        "    # Calcular el porcentaje de d√≠as incompletos\n",
        "    total_dias = len(conteo_diario)\n",
        "    dias_con_menos = (conteo_diario < registros_dia_completo).sum()\n",
        "    porcentaje = (dias_con_menos / total_dias) * 100\n",
        "\n",
        "    print(f\"D√≠as con menos de {registros_dia_completo} registros: {dias_con_menos} de {total_dias} ({porcentaje:.2f}%)\")\n",
        "\n",
        "    return conteo_diario, registros_dia_completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "15b8e1dd",
      "metadata": {
        "id": "15b8e1dd",
        "outputId": "086beb93-a6e5-4772-ee51-3d2bb2470592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de cantidad de registros por d√≠a:\n",
            "\n",
            "+---------------------+--------------------+\n",
            "|   Registros por d√≠a |   Cantidad de d√≠as |\n",
            "+=====================+====================+\n",
            "|                 511 |               1198 |\n",
            "+---------------------+--------------------+\n",
            "|                 510 |                  8 |\n",
            "+---------------------+--------------------+\n",
            "|                 509 |                  4 |\n",
            "+---------------------+--------------------+\n",
            "|                 508 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 504 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 503 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 502 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 500 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 499 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 458 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 432 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 404 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 394 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 383 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 346 |                  9 |\n",
            "+---------------------+--------------------+\n",
            "|                 344 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 122 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 121 |                  9 |\n",
            "+---------------------+--------------------+\n",
            "|                 120 |                  3 |\n",
            "+---------------------+--------------------+\n",
            "|                 119 |                  2 |\n",
            "+---------------------+--------------------+\n",
            "|                 118 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 117 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 116 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 109 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "|                 108 |                  1 |\n",
            "+---------------------+--------------------+\n",
            "\n",
            "Cantidad de registros en un d√≠a completo: 511\n",
            "D√≠as con menos de 511 registros: 56 de 1254 (4.47%)\n"
          ]
        }
      ],
      "source": [
        "resumen, registros_dia_completo = analizar_registros_por_dia(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cef1314",
      "metadata": {
        "id": "5cef1314"
      },
      "source": [
        "Como podemos observar en la tabla, la gran mayor√≠a de d√≠as tienen `511` muestras. Y representan m√°s del 95% del total de los datos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aee7dba",
      "metadata": {
        "id": "3aee7dba"
      },
      "source": [
        "### 5.1. Filtrado de d√≠as incompletos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc60a994",
      "metadata": {
        "id": "fc60a994"
      },
      "source": [
        "La siguiente funci√≥n encuentra los indices de las fechas con registros incompletos (menor a 511):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "58057ccf",
      "metadata": {
        "id": "58057ccf"
      },
      "outputs": [],
      "source": [
        "def busqueda_fechas_incompletas(df: pd.DataFrame, gap_minutes: int = 1, registros_esperados: int = registros_dia_completo) -> list:\n",
        "    \"\"\"\n",
        "    Muestra una tabla con los d√≠as que tienen menos de los registros esperados o presentan irregularidades temporales.\n",
        "    Retorna una lista con esas fechas.\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame con √≠ndice datetime.\n",
        "    - gap_minutes: intervalo esperado entre registros consecutivos (en minutos).\n",
        "    - registros_esperados: cantidad esperada de registros por d√≠a.\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de fechas (datetime.date) con menos registros de los esperados o problemas temporales.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_diff'] = df.index.to_series().diff()\n",
        "    base_time_diff = pd.Timedelta(minutes=gap_minutes)\n",
        "\n",
        "    conteos = df.groupby(df.index.date).size()\n",
        "    fechas_problema = []\n",
        "\n",
        "    for date, group in df.groupby(df.index.date):\n",
        "        time_diff = group['time_diff'].iloc[1:]\n",
        "        tiene_irregularidades = (time_diff != base_time_diff).any()\n",
        "        cantidad = conteos[date]\n",
        "\n",
        "        if cantidad < registros_esperados or tiene_irregularidades:\n",
        "            fechas_problema.append((date, cantidad))\n",
        "\n",
        "    '''\n",
        "    if fechas_problema:\n",
        "        print(f\"\\nD√≠a con menos de {registros_esperados} registros o con problemas temporales:\\n\")\n",
        "        print(f\"{'+' + '-'*21 + '+' + '-'*20 + '+'}\")\n",
        "        print(f\"| {'Fecha'.ljust(19)} | {'Registros'.rjust(18)} |\")\n",
        "        print(f\"{'+' + '='*21 + '+' + '='*20 + '+'}\")\n",
        "        for fecha, registros in fechas_problema:\n",
        "            print(f\"| {str(fecha).ljust(19)} | {str(registros).rjust(18)} |\")\n",
        "            print(f\"{'+' + '-'*21 + '+' + '-'*20 + '+'}\")\n",
        "    else:\n",
        "        print(\"No se encontraron d√≠as con irregularidades ni registros incompletos.\")\n",
        "    '''\n",
        "    # Solo devolver las fechas\n",
        "    return [fecha for fecha, _ in fechas_problema]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f354689",
      "metadata": {
        "id": "3f354689"
      },
      "source": [
        "Elimino las fechas con registros incompletos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "53934801",
      "metadata": {
        "id": "53934801",
        "outputId": "0bf6f2bd-fc94-4e5f-9680-4d5dae918131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribuci√≥n de cantidad de registros por d√≠a:\n",
            "\n",
            "+---------------------+--------------------+\n",
            "|   Registros por d√≠a |   Cantidad de d√≠as |\n",
            "+=====================+====================+\n",
            "|                 511 |               1198 |\n",
            "+---------------------+--------------------+\n",
            "\n",
            "Cantidad de registros en un d√≠a completo: 511\n",
            "D√≠as con menos de 511 registros: 0 de 1198 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# Filtrar eliminando las fechas con problemas\n",
        "df_mnq = df_mnq[~df_mnq.index.to_series().dt.date.isin(busqueda_fechas_incompletas(df_mnq))]\n",
        "\n",
        "#Y verifico con:\n",
        "resumen=analizar_registros_por_dia(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5be8e7",
      "metadata": {
        "id": "2e5be8e7"
      },
      "source": [
        "## 6. Verificaci√≥n de continuidad temporal minuto a minuto"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e80f60",
      "metadata": {
        "id": "23e80f60"
      },
      "source": [
        "Es necesario verificar que los 451 registros correspondientes a un mismo d√≠a est√©n dispuestos de forma consecutiva, con una separaci√≥n exacta de un minuto entre cada muestra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2c5a40c1",
      "metadata": {
        "id": "2c5a40c1"
      },
      "outputs": [],
      "source": [
        "def detectar_gaps(df: pd.DataFrame, gap_minutes: int = 1):\n",
        "    \"\"\"\n",
        "    Verifica si existen saltos mayores al intervalo esperado (por defecto 1 minuto)\n",
        "    entre registros consecutivos dentro de cada d√≠a, en un DataFrame con √≠ndice tipo DatetimeIndex.\n",
        "\n",
        "    Omite el primer registro de cada d√≠a.\n",
        "\n",
        "    Par√°metros:\n",
        "    - df: DataFrame con √≠ndice datetime.\n",
        "    - gap_minutes: tama√±o esperado del intervalo en minutos (por defecto 1).\n",
        "\n",
        "    Retorna:\n",
        "    - Lista de √≠ndices donde se detectaron diferencias mayores al intervalo esperado.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['time_diff'] = df.index.to_series().diff()\n",
        "\n",
        "    base_time_diff = pd.Timedelta(minutes=gap_minutes)\n",
        "    problem_indices = []\n",
        "\n",
        "    for date, group in df.groupby(df.index.date):\n",
        "        time_diff = group['time_diff'].iloc[1:]\n",
        "        incorrect_indices = time_diff[time_diff != base_time_diff].index\n",
        "        if len(incorrect_indices) > 0:\n",
        "            problem_indices.append(incorrect_indices)\n",
        "\n",
        "    if problem_indices:\n",
        "        print(f\"Se encontraron problemas en {len(problem_indices)} registros con diferencias irregulares.\\n\")\n",
        "\n",
        "        # Conteo por fecha\n",
        "        conteos = df.groupby(df.index.date).size()\n",
        "\n",
        "        for i in range(len(problem_indices)):\n",
        "            idx = problem_indices[i][0]\n",
        "            diff = df.loc[idx, 'time_diff']\n",
        "            date = idx.date()\n",
        "            count = conteos[date]\n",
        "            print(f'\\t{idx} -> Diferencia: {diff} | # Registros: {count}')\n",
        "    else:\n",
        "        print(\"No se encontraron problemas, todas las muestras son consecutivas minuto a minuto.\")\n",
        "\n",
        "    return problem_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "9136b7ba",
      "metadata": {
        "id": "9136b7ba",
        "outputId": "74f21389-4b47-4fac-a952-1d39008654b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se encontraron problemas, todas las muestras son consecutivas minuto a minuto.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "detectar_gaps(df_mnq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be37b70",
      "metadata": {
        "id": "9be37b70"
      },
      "source": [
        "## 7. Guardado de dataset final\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6901eb",
      "metadata": {
        "id": "0b6901eb"
      },
      "source": [
        "Se guarda un dataset compuesto por 1198 d√≠as, cada uno con 511 registros correspondientes a minutos consecutivos.\n",
        "\n",
        "El conjunto de datos incluye √∫nicamente d√≠as h√°biles de operaci√≥n burs√°til, y abarca el intervalo horario comprendido entre las 07:30 y las 16:00 horas (US/Eastern)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardamos el dataset\n",
        "ruta_mnq_intraday = '/content/neural_profit/2_obtencion_preparacion_exploracion_datos/mnq_intraday_data.parquet'\n",
        "df_mnq.to_parquet(ruta_mnq_intraday, index=True)"
      ],
      "metadata": {
        "id": "ks7D-6wsT839"
      },
      "id": "ks7D-6wsT839",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cambiamos de directorio\n",
        "%cd neural_profit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqs_Th8Mmtev",
        "outputId": "35dae696-9dca-4607-cb75-98b4fbb72ade"
      },
      "id": "nqs_Th8Mmtev",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neural_profit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token = \" \" #busca el archivo token en D:\\Escritorio\\UBA-CEIA\\token_neural_profit\n",
        "!git config --global user.email \"gusunapillco@gmail.com\"\n",
        "!git config --global user.name \"GUNAPILLCO\"\n",
        "!git add .\n",
        "!git commit -m \"Actualizaci√≥n de datasets desde colab\"\n",
        "!git push https://GUNAPILLCO:{token}@github.com/GUNAPILLCO/neural_profit.git\n",
        "\n",
        "print(\"Dataset guardados correctamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnoC50l5malj",
        "outputId": "c85f60be-7166-4d49-ee39-1d284915888e"
      },
      "id": "nnoC50l5malj",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 6f1cbab] Actualizaci√≥n de notebooks y datasets desde colab\n",
            " 2 files changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 2_obtencion_preparacion_exploracion_datos/mnq_intraday_data.parquet\n",
            " create mode 100644 2_obtencion_preparacion_exploracion_datos/mnq_raw_data.parquet\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 29.94 MiB | 4.55 MiB/s, done.\n",
            "Total 5 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/GUNAPILLCO/neural_profit.git\n",
            "   aab5afd..6f1cbab  main -> main\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "trader_IA",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}